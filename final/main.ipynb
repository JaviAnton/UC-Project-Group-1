{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import osmnx as ox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load map\n",
    "g=ox.graph_from_place('Paris, France', network_type='bike',simplify=False)\n",
    "gdf_nodes, gdf_edges=ox.graph_to_gdfs(g)\n",
    "\n",
    "#Prune road types\n",
    "remove_tags=['bridleway','path','unclassified','road']\n",
    "\n",
    "del_list=[]\n",
    "for j,i in gdf_edges.iterrows():\n",
    "    if i[\"highway\"] in remove_tags:\n",
    "        del_list.append(j)\n",
    "gdf_edges=gdf_edges.drop(index=del_list)\n",
    "g=ox.utils_graph.graph_from_gdfs(gdf_nodes, gdf_edges, graph_attrs=None)  \n",
    "\n",
    "#Rebuild the map\n",
    "largest_cc = max(nx.strongly_connected_components(g), key=len)\n",
    "g = g.subgraph(largest_cc)\n",
    "gdf_nodes, gdf_edges=ox.graph_to_gdfs(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataGenerator import DataGenerator\n",
    "import csv\n",
    "import ast\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(samplings,tracks,append=False):\n",
    "    mode = \"a\" if append else \"w\"\n",
    "\n",
    "    with open(\"data/generated-data-samplings.csv\", mode) as f:  \n",
    "        writer = csv.writer(f, delimiter=';', quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "        for sampling in samplings:\n",
    "            writer.writerow(sampling)\n",
    "\n",
    "    with open(\"data/generated-data-tracks.csv\", mode) as f:  \n",
    "        writer = csv.writer(f, delimiter=';', quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "        for track in tracks:\n",
    "            writer.writerow(track)\n",
    "\n",
    "def load_data():\n",
    "    with open('data/generated-data-samplings.csv', 'r') as f:\n",
    "        samplings=[]\n",
    "        for sampling in f:\n",
    "            entries = sampling.strip().split(';')\n",
    "            nested_lists = [ast.literal_eval(entry) for entry in entries]\n",
    "            samplings.append(nested_lists)\n",
    "\n",
    "    with open('data/generated-data-tracks.csv', 'r') as f:\n",
    "        tracks=[]\n",
    "        for track in f:\n",
    "            entries = track.strip().split(';')\n",
    "            nested_lists = [ast.literal_eval(entry) for entry in entries]\n",
    "            tracks.append(nested_lists)\n",
    "    return samplings, tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data generation\n"
     ]
    }
   ],
   "source": [
    "#Create the generator\n",
    "path=\"data/comptage-velo-donnees-compteurs.csv\"\n",
    "datagen=DataGenerator(G=g,gdf_nodes_edges=(gdf_nodes,gdf_edges),data_path=path)\n",
    "\n",
    "#Generate data in bacthes and save it to the generated data\n",
    "print(\"Data generation\")\n",
    "generator_batch_size=100\n",
    "n_batches=10\n",
    "for batch in range(n_batches):\n",
    "    samplings,tracks=datagen.track_generator(10,sampling_rate=1)\n",
    "    save_data(samplings,tracks,append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the generated data\n",
    "phi,T=load_data()\n",
    "\n",
    "phi_train,phi_test,T_train,T_test=train_test_split(phi,T,train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Travel time distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_TTD = ox.add_edge_speeds(g)\n",
    "baseline_TTD = ox.add_edge_travel_times(baseline_TTD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectory recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edges_to_nodes(track):\n",
    "    return [edge[0] for edge in track]\n",
    "\n",
    "def nodes_to_edges(track):\n",
    "    return [(track[i],track[i+1]) for i in range(len(track)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_route(track_sampling,recovery_algorithm,debug=False):\n",
    "\n",
    "    edges= [(n1,n2) for t,n1,n2,e in track_sampling]\n",
    "    unique_edges = list(OrderedDict.fromkeys(edges))\n",
    "\n",
    "    edge_0=unique_edges[0]\n",
    "    recovered_track=[edge_0]\n",
    "    n_added_edges=0\n",
    "    for edge_1 in unique_edges[1:]:\n",
    "        if edge_1[0]!=edge_0[1]:\n",
    "            recovered_path=recovery_algorithm(edge_0[1],edge_1[0])\n",
    "            recovered_track+=recovered_path\n",
    "            if debug:\n",
    "                # print(f'Adding {len(recovered_path)} edges between {edge_0[1]} and {edge_1[0]}')\n",
    "                n_added_edges+=len(recovered_path)\n",
    "                \n",
    "        recovered_track.append(edge_1)\n",
    "        edge_0=edge_1\n",
    "\n",
    "    if debug:\n",
    "        print(f'Recovered track consists on {len(recovered_track)} edges, of which {n_added_edges} ({100*n_added_edges/len(recovered_track):.0f}%) were added.')\n",
    "\n",
    "    return recovered_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_routes_from_samplings(samplings,recovery_algorithm,debug=False):\n",
    "    return [recover_route(s,recovery_algorithm,debug=debug) for s in samplings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_recovery_algorithm(n1,n2):\n",
    "    recovered_path=nx.shortest_path(g, n1, n2, weight='length')\n",
    "    return nodes_to_edges(recovered_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_TTD_recovery_algorithm(n1,n2):\n",
    "    recovered_path=nx.shortest_path(baseline_TTD, n1, n2, weight='travel_time')\n",
    "    return nodes_to_edges(recovered_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_hat_baseline=recover_routes_from_samplings(phi_test,baseline_recovery_algorithm)\n",
    "\n",
    "T_hat_baseline_TTD=recover_routes_from_samplings(phi_test,baseline_TTD_recovery_algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn_extra.cluster import KMedoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_wise_track_similarity(G,T):\n",
    "    M=np.zeros((len(T),len(T)))\n",
    "    G_aux=G.copy()\n",
    "\n",
    "    for i,track_i in enumerate(T):\n",
    "\n",
    "        G_aux.add_node(\"vSource\")\n",
    "        G_aux.add_edges_from((\"vSource\", n, {'length': 0}) for n in track_i)\n",
    "        dists=nx.single_source_shortest_path_length(G_aux,\"vSource\")\n",
    "        G_aux.remove_node(\"vSource\")\n",
    "\n",
    "        for j,track_j in enumerate(T):\n",
    "            if i==j:\n",
    "                continue\n",
    "            \n",
    "            max_dist=max(dists[m] for m in track_j)\n",
    "                    \n",
    "            M[i,j]=max_dist\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetrize(M,strategy='min'):\n",
    "    if strategy=='mean':\n",
    "        return (1/2)*(M+M.T)\n",
    "    if strategy=='min':\n",
    "        return np.minimum(M,M.T)\n",
    "    if strategy=='max':\n",
    "        return np.maximum(M,M.T)\n",
    "    raise KeyError(f'{strategy} is not a valid strategy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_corridors_medoid_summarizer(M,k,T,**kwargs):\n",
    "    kmedoids = KMedoids(n_clusters=k, metric='precomputed', **kwargs) #For example random_state=0\n",
    "    kmedoids.fit(M)\n",
    "\n",
    "    cluster_labels = kmedoids.labels_\n",
    "    medoid_indices = kmedoids.medoid_indices_\n",
    "    d=kmedoids.inertia_\n",
    "\n",
    "    k_corridors=[T[i] for i in medoid_indices]\n",
    "    return k_corridors,medoid_indices,cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_dict={}\n",
    "paths_dict={}\n",
    "\n",
    "#Compute the similarity matrix\n",
    "paths_hat_baseline=[edges_to_nodes(track) for track in T_hat_baseline]\n",
    "M_baseline=row_wise_track_similarity(g,paths_hat_baseline)\n",
    "M_baseline=symmetrize(M_baseline,strategy='min')\n",
    "M_dict['baseline']=M_baseline\n",
    "paths_dict['baseline']=paths_hat_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters=1\n",
    "\n",
    "clustering_dict={}\n",
    "for key,M in M_dict.items():\n",
    "    T=paths_dict[key]\n",
    "    clustering=k_corridors_medoid_summarizer(M,n_clusters,T,random_state=42)\n",
    "    clustering_dict[key]=clustering #k_corridors,k_index,cluster_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-metrics\n",
    "For k-metrics, we compare an evaluation parameter with _all_ k corridors at once. I.e., for a track we don't evaluate if there is an intersection with a corridor but with _all_ of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_absolute_intersect(track_i,k_tracks):\n",
    "    \"\"\"\n",
    "    Returns 1 if there is at least one edge from track_i present in k_tracks,\n",
    "    and 0 otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    #Convert list of nodes into list of edges\n",
    "    set_i=set(nodes_to_edges(track_i))\n",
    "\n",
    "    for track_j in k_tracks:\n",
    "        set_j=set(nodes_to_edges(track_j))\n",
    "\n",
    "        #Calculate intesection\n",
    "        intersect=set_i.intersection(set_j)\n",
    "        n=len(intersect)\n",
    "        if n!=1:\n",
    "            return 1\n",
    "    \n",
    "    return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_relative_intersect(track_i,k_tracks):\n",
    "    \"\"\"\n",
    "    Returns what portion of the edges from track_i are present in k_tracks\n",
    "    \"\"\"\n",
    "\n",
    "    #Convert list of nodes into list of edges\n",
    "    set_i=set(nodes_to_edges(track_i))\n",
    "    k_edges=np.concatenate([nodes_to_edges(track_j) for track_j in k_tracks])\n",
    "    set_j=set([(n1,n2) for n1,n2 in k_edges])\n",
    "\n",
    "    #Calculate intesection\n",
    "    intersect=set_i.intersection(set_j)\n",
    "    n=len(intersect)\n",
    "    \n",
    "    return n/len(set_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_lenght_relative_intersect(track_i,k_tracks,G):\n",
    "    \"\"\"\n",
    "    Returns what portion of the length of track_i is in the cummulative length of the edges\n",
    "    also present in k_tracks\n",
    "    \"\"\"\n",
    "\n",
    "    #Convert list of nodes into list of edges\n",
    "    set_i=set(nodes_to_edges(track_i))\n",
    "    k_edges=np.concatenate([nodes_to_edges(track_j) for track_j in k_tracks])\n",
    "    set_j=set([(n1,n2) for n1,n2 in k_edges])\n",
    "\n",
    "    #Calculate intersection\n",
    "    intersect=set_i.intersection(set_j)\n",
    "\n",
    "    l_intersect=sum([G.get_edge_data(*edge)[0]['length'] for edge in intersect])\n",
    "    l_i=sum([G.get_edge_data(*edge)[0]['length'] for edge in set_i])\n",
    "\n",
    "\n",
    "    \n",
    "    return l_intersect/l_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_k_metric(metric_fun,T,k_corridors,**kwargs):\n",
    "    \"\"\"\n",
    "    Returs the cummulative value (normalized by the number of tracks) of the \n",
    "    selected metric for the selected k corridors.\n",
    "    T: A list of tracks to be evaluated (if k_corridors are included they will also be evaluated)\n",
    "    k_corridors: A list of k selected corridors\n",
    "    metric_fun: The metric function to evaluate as metric_fun(T[i],k_corridors[j],**kwargs)\n",
    "    \"\"\"\n",
    "\n",
    "    result=0\n",
    "    for track_i in T:\n",
    "        result+=metric_fun(track_i,k_corridors,**kwargs)\n",
    "\n",
    "    result/=len(T)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M-metrics\n",
    "M-metrics apply a matrix operation to the set of tracks and clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_M(G,T,cluster_labels,k_index,sym_strategy=None):\n",
    "        M=row_wise_track_similarity(G,T)\n",
    "        if sym_strategy:\n",
    "            M=symmetrize(M,strategy=sym_strategy)\n",
    "\n",
    "        d=np.min(M[:,k_index],axis=1).sum()\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_similarity(G,T,cluster_labels,k_index,include_corridors=False,ponderated=True,ponderation_k=1):\n",
    "    '''\n",
    "    This metric returns a metric based on the average similarity distance from\n",
    "    the tracks to the corridors.\n",
    "    If include_corridors, the average is made taking into consideration the corridors\n",
    "    as tracks too, otherwise the average is made using only the rest of the tracks.\n",
    "    \n",
    "    If ponderated, the result is ponderated so that the metric ranges (0,1],\n",
    "    where 1 means all tracks are corridors and 0 tracks are at an infinite distance from the corridors.\n",
    "    The ponderation is k/(k+d), where k is the ponderation parameter that defines at which d\n",
    "    the ponderated score is equal to 1/2\n",
    "    '''\n",
    "\n",
    "    d=evaluate_M(G,T,cluster_labels,k_index)\n",
    "    n=len(T)\n",
    "    if not include_corridors:\n",
    "        n-=len(k_index)\n",
    "    d/=n\n",
    "    if ponderated:\n",
    "        return ponderation_k/(ponderation_k+d)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------------\n",
      "\tbaseline\t\n",
      "------------------\n",
      "\n",
      "k-set metrics:\n",
      "--------------\n",
      "Absolute intersection: 1.00\n",
      "Relative intersection: 0.50\n",
      "Length intersection: 0.50\n",
      "\n",
      "M metrics:\n",
      "----------\n",
      "M average distance: 118m\n",
      "Ponderated M similarity: 0.0084\n"
     ]
    }
   ],
   "source": [
    "for key,clustering in clustering_dict.items():\n",
    "    k_corridors,k_index,cluster_labels=clustering\n",
    "\n",
    "    print(f'\\n\\n------------------\\n\\t{key}\\t\\n------------------')\n",
    "\n",
    "    print(\"\\nk-set metrics:\\n--------------\")\n",
    "\n",
    "    abs_inter=evaluate_k_metric(k_absolute_intersect,T,k_corridors)\n",
    "    print(\"Absolute intersection: {:.2f}\".format(abs_inter))\n",
    "\n",
    "    rel_inter=evaluate_k_metric(k_relative_intersect,T,k_corridors)\n",
    "    print(\"Relative intersection: {:.2f}\".format(rel_inter))\n",
    "\n",
    "    l_rel_inter=evaluate_k_metric(k_lenght_relative_intersect,T,k_corridors,G=g)\n",
    "    print(\"Length intersection: {:.2f}\".format(l_rel_inter))\n",
    "\n",
    "    print(\"\\nM metrics:\\n----------\")\n",
    "\n",
    "    m_sim=M_similarity(g,T,cluster_labels,k_index,ponderated=False,include_corridors=True)\n",
    "    print(\"M average distance: {:.0f}m\".format(m_sim))\n",
    "\n",
    "    m_sim=M_similarity(g,T,cluster_labels,k_index,ponderated=True,include_corridors=True)\n",
    "    print(\"Ponderated M similarity: {:.4f}\".format(m_sim))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
