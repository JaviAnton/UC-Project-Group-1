{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'base'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Shaoxuan Zhang\\Documents\\GitHub\\UC-Project-Group-1\\GCN train.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Shaoxuan%20Zhang/Documents/GitHub/UC-Project-Group-1/GCN%20train.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdgl\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Shaoxuan%20Zhang/Documents/GitHub/UC-Project-Group-1/GCN%20train.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mutils\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Shaoxuan%20Zhang/Documents/GitHub/UC-Project-Group-1/GCN%20train.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbase\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Shaoxuan%20Zhang/Documents/GitHub/UC-Project-Group-1/GCN%20train.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Shaoxuan%20Zhang/Documents/GitHub/UC-Project-Group-1/GCN%20train.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'base'"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import utils\n",
    "import base\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from dgl.nn.pytorch import RelGraphConv\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relational Graph Covo Networks (R-GCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGCN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes,\n",
    "        h_dim,\n",
    "        out_dim,\n",
    "        num_rels,\n",
    "        regularizer=\"basis\",\n",
    "        num_bases=-1,\n",
    "        dropout=0.0,\n",
    "        self_loop=False,\n",
    "        ns_mode=False,\n",
    "    ):\n",
    "        super(RGCN, self).__init__()\n",
    "\n",
    "        if num_bases == -1:\n",
    "            num_bases = num_rels\n",
    "        self.emb = nn.Embedding(num_nodes, h_dim)\n",
    "        self.conv1 = RelGraphConv(\n",
    "            h_dim, h_dim, num_rels, regularizer, num_bases, self_loop=self_loop\n",
    "        )\n",
    "        self.conv2 = RelGraphConv(\n",
    "            h_dim,\n",
    "            out_dim,\n",
    "            num_rels,\n",
    "            regularizer,\n",
    "            num_bases,\n",
    "            self_loop=self_loop,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ns_mode = ns_mode\n",
    "\n",
    "    def forward(self, g, nids=None):\n",
    "        if self.ns_mode:\n",
    "            # forward for neighbor sampling\n",
    "            x = self.emb(g[0].srcdata[dgl.NID])\n",
    "            h = self.conv1(g[0], x, g[0].edata[dgl.ETYPE], g[0].edata[\"norm\"])\n",
    "            h = self.dropout(F.relu(h))\n",
    "            h = self.conv2(g[1], h, g[1].edata[dgl.ETYPE], g[1].edata[\"norm\"])\n",
    "            return h\n",
    "        else:\n",
    "            x = self.emb.weight if nids is None else self.emb(nids)\n",
    "            h = self.conv1(g, x, g.edata[dgl.ETYPE], g.edata[\"norm\"])\n",
    "            h = self.dropout(F.relu(h))\n",
    "            h = self.conv2(g, h, g.edata[dgl.ETYPE], g.edata[\"norm\"])\n",
    "            return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2036066188.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [20], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    num_nodes =  # Specify the number of nodes\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "num_nodes =  # Specify the number of nodes\n",
    "h_dim =  # Specify the input feature dimension\n",
    "out_dim =  # Specify the output dimension\n",
    "num_rels = 5 # Specify the number of relation types in your graph\n",
    "regularizer = \"basis\"  # You can change this based on your requirements\n",
    "num_bases = -1  # You can change this based on your requirements\n",
    "dropout = 0.0  # You can change this based on your requirements\n",
    "self_loop = False  # You can change this based on your requirements\n",
    "ns_mode = False  # You can change this based on your requirements\n",
    "\n",
    "# Instantiate the RGCN model\n",
    "model = RGCN(\n",
    "    num_nodes=num_nodes,\n",
    "    h_dim=h_dim,\n",
    "    out_dim=out_dim,\n",
    "    num_rels=num_rels,\n",
    "    regularizer=regularizer,\n",
    "    num_bases=num_bases,\n",
    "    dropout=dropout,\n",
    "    self_loop=self_loop,\n",
    "    ns_mode=ns_mode,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Train 1 - Assign Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation Metrics\n",
    "def hetro_loss(x, mu, v):\n",
    "    return (((x - mu) ** 2 / v) + torch.log(v)).mean()\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = y_true.cpu().numpy(), y_pred.cpu().numpy()\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def compute_rmse(y_true, y_pred):\n",
    "    y_true, y_pred = y_true.cpu().numpy(), y_pred.cpu().numpy()\n",
    "    return np.sqrt(((y_true/60 - y_pred/60) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,optimizer,epoch,batch_size, train_data):\n",
    "\n",
    "    train_path, train_ratio, train_slots, train_orderid, train_label =train_data\n",
    "    road_idx=torch.tensor(list(range(num_locations))).to(device)\n",
    "    time_idx=torch.tensor(list(range(num_times))).to(device)       \n",
    "    model.train()\n",
    "    tte_loss,speed_loss=0.0,0.0,0.0,0.0,0.0\n",
    "    for i in range(0, train_path.shape[0],batch_size):\n",
    "        paths_temp,ratio_temp,time_temp=train_path[i:i+batch_size],train_ratio[i:i+batch_size],train_slots[i:i+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        entire_out,mu,sigma=model(road_idx,time_idx,time_temp,g2,u,lengths,paths_temp,edge_type)\n",
    "            # add 1\n",
    "        \n",
    "        mu=torch.cat([mu,torch.zeros(1,mu.shape[1]).to(device)])\n",
    "        sigma=torch.cat([sigma,torch.zeros(1,sigma.shape[1]).to(device)])\n",
    "        t=train_slots[i:i+batch_size]\n",
    "        \n",
    "        path_mu,path_sigma=mu[paths_temp],sigma[paths_temp]\n",
    "        path_mu_new,path_mu_path_sigma=[],[]\n",
    "        for o,l in enumerate(t):\n",
    "            path_mu_new.append(path_mu[o:o+1,:,l])\n",
    "            path_mu_path_sigma.append(path_sigma[o:o+1,:,l])\n",
    "        path_mu,path_sigma=torch.cat(path_mu_new),torch.cat(path_mu_path_sigma)\n",
    "    \n",
    "        E_mu=torch.mul(path_mu,ratio_temp).sum(dim=1)+eps\n",
    "        E_sigma=torch.mul(path_sigma,ratio_temp).pow(2).sum(dim=1)+eps\n",
    "\n",
    "    \n",
    "        label_time=train_label[i:i+batch_size,0:1]\n",
    "        label_speed=train_label[i:i+batch_size,1]\n",
    "        loss1=hetro_loss(label_speed,E_mu,E_sigma)\n",
    "        loss2 = (torch.abs(entire_out - label_time) / label).mean()\n",
    "\n",
    "        tte_loss+=torch.abs(entire_out-label_time).sum()\n",
    "        speed_loss+=torch.abs(label_speed-E_mu).sum()\n",
    "        \n",
    "        print('\\r tte loss: %f speed loss %f' %(tte_loss.item(), speed_loss), end=\"\")\n",
    "\n",
    "        (loss1+loss2).mean().backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print()\n",
    "    print(\"Epoch:\",epoch, tte_loss.item(),speed_loss.item())\n",
    "    return tte_loss,speed_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_traveltime(model, epoch,batch_size, test_data):\n",
    "    model.eval()\n",
    "    tte_loss,total_output,test_samples=0.0,0.0,[],0.0\n",
    "    test_path,test_ratio,test_slots,test_orderid,test_label=test_data\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, test_path.shape[0],batch_size):\n",
    "            \n",
    "            paths_temp,ratios_temp,time_temp=test_path[i:i+batch_size],test_ratio[i:i+batch_size],test_slots[i:i+batch_size]\n",
    "            test_samples+=paths_temp.shape[0]\n",
    "            entire_out,mu,sigma=model(road_idx,time_idx,time_temp,g2,u,lengths,paths_temp,edge_type)\n",
    "\n",
    "            label_time=test_label[i:i+batch_size,0:1]\n",
    "            total_output.append(entire_out)\n",
    "            tte_loss+=torch.abs(entire_out-label_time).sum().item()\n",
    "        tte_loss/=test_samples\n",
    "        total_output=torch.cat(total_output)\n",
    "        index=torch.unique(test_orderid)\n",
    "        traj_output,label=torch.zeros(len(index),1),torch.zeros(len(index),1)\n",
    "        for o, idx in enumerate(index):\n",
    "            #max_lens=max(len(idx.reshape(-1)),max_lens)\n",
    "            traj_output[o],label[o]=total_output[test_orderid==idx].sum(),test_label[test_orderid==idx].sum()\n",
    "        traj_tte_loss=torch.abs(traj_output-label).mean()\n",
    "       \n",
    "        mse = compute_rmse(label, traj_output)\n",
    "        mape = mean_absolute_percentage_error(label, traj_output)\n",
    "        print(\"\\n Epoch: %d, Path tte loss: %.4f, traj_tte_loss: %.4f\" %( epoch,tte_loss/60,traj_tte_loss/60))\n",
    "        return tte_loss,traj_tte_loss,mse,mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Train 2 - Deep TTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, kernel_size = 3, num_filter = 32, pooling_method = 'attention', num_final_fcs = 3, final_fc_size = 128, alpha = 0.3):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # parameter of attribute / spatio-temporal component\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_filter = num_filter\n",
    "        self.pooling_method = pooling_method\n",
    "\n",
    "        # parameter of multi-task learning component\n",
    "        self.num_final_fcs = num_final_fcs\n",
    "        self.final_fc_size = final_fc_size\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.build()\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if name.find('.bias') != -1:\n",
    "                param.data.fill_(0)\n",
    "            elif name.find('.weight') != -1:\n",
    "                nn.init.xavier_uniform(param.data)\n",
    "\n",
    "    def build(self):\n",
    "        # attribute component\n",
    "        self.attr_net = base.Attr.Net()\n",
    "\n",
    "        # spatio-temporal component\n",
    "        self.spatio_temporal = base.SpatioTemporal.Net(attr_size = self.attr_net.out_size(), \\\n",
    "                                                       kernel_size = self.kernel_size, \\\n",
    "                                                       num_filter = self.num_filter, \\\n",
    "                                                       pooling_method = self.pooling_method\n",
    "        )\n",
    "\n",
    "        self.entire_estimate = EntireEstimator(input_size =  self.spatio_temporal.out_size() + self.attr_net.out_size(), num_final_fcs = self.num_final_fcs, hidden_size = self.final_fc_size)\n",
    "\n",
    "        self.local_estimate = LocalEstimator(input_size = self.spatio_temporal.out_size())\n",
    "\n",
    "\n",
    "    def forward(self, attr, traj, config):\n",
    "        attr_t = self.attr_net(attr)\n",
    "\n",
    "        # sptm_s: hidden sequence (B * T * F); sptm_l: lens (list of int); sptm_t: merged tensor after attention/mean pooling\n",
    "        sptm_s, sptm_l, sptm_t = self.spatio_temporal(traj, attr_t, config)\n",
    "\n",
    "        entire_out = self.entire_estimate(attr_t, sptm_t)\n",
    "\n",
    "        # sptm_s is a packed sequence (see pytorch doc for details), only used during the training\n",
    "        if self.training:\n",
    "            local_out = self.local_estimate(sptm_s[0])\n",
    "            return entire_out, (local_out, sptm_l)\n",
    "        else:\n",
    "            return entire_out\n",
    "\n",
    "    def eval_on_batch(self, attr, traj, config):\n",
    "        if self.training:\n",
    "            entire_out, (local_out, local_length) = self(attr, traj, config)\n",
    "        else:\n",
    "            entire_out = self(attr, traj, config)\n",
    "\n",
    "        pred_dict, entire_loss = self.entire_estimate.eval_on_batch(entire_out, attr['time'], config['time_mean'], config['time_std'])\n",
    "\n",
    "        if self.training:\n",
    "            # get the mean/std of each local path\n",
    "            mean, std = (self.kernel_size - 1) * config['time_gap_mean'], (self.kernel_size - 1) * config['time_gap_std']\n",
    "\n",
    "            # get ground truth of each local path\n",
    "            local_label = utils.get_local_seq(traj['time_gap'], self.kernel_size, mean, std)\n",
    "            local_loss = self.local_estimate.eval_on_batch(local_out, local_length, local_label, mean, std)\n",
    "\n",
    "            return pred_dict, (1 - self.alpha) * entire_loss + self.alpha * local_loss\n",
    "        else:\n",
    "            return pred_dict, entire_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 10\n",
    "\n",
    "class EntireEstimator(nn.Module):\n",
    "    def __init__(self, input_size, num_final_fcs, hidden_size = 128):\n",
    "        super(EntireEstimator, self).__init__()\n",
    "\n",
    "        self.input2hid = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        self.residuals = nn.ModuleList()\n",
    "        for i in range(num_final_fcs):\n",
    "            self.residuals.append(nn.Linear(hidden_size, hidden_size))\n",
    "\n",
    "        self.hid2out = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, attr_t, sptm_t):\n",
    "        inputs = torch.cat((attr_t, sptm_t), dim = 1)\n",
    "\n",
    "        hidden = F.leaky_relu(self.input2hid(inputs))\n",
    "\n",
    "        for i in range(len(self.residuals)):\n",
    "            residual = F.leaky_relu(self.residuals[i](hidden))\n",
    "            hidden = hidden + residual\n",
    "\n",
    "        out = self.hid2out(hidden)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def eval_on_batch(self, pred, label, mean, std):\n",
    "        label = label.view(-1, 1)\n",
    "\n",
    "        label = label * std + mean\n",
    "        pred = pred * std + mean\n",
    "\n",
    "        loss = torch.abs(pred - label) / label\n",
    "\n",
    "        return {'label': label, 'pred': pred}, loss.mean()\n",
    "\n",
    "class LocalEstimator(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LocalEstimator, self).__init__()\n",
    "\n",
    "        self.input2hid = nn.Linear(input_size, 64)\n",
    "        self.hid2hid = nn.Linear(64, 32)\n",
    "        self.hid2out = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, sptm_s):\n",
    "        hidden = F.leaky_relu(self.input2hid(sptm_s))\n",
    "\n",
    "        hidden = F.leaky_relu(self.hid2hid(hidden))\n",
    "\n",
    "        out = self.hid2out(hidden)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def eval_on_batch(self, pred, lens, label, mean, std):\n",
    "        label = nn.utils.rnn.pack_padded_sequence(label, lens, batch_first = True)[0]\n",
    "        label = label.view(-1, 1)\n",
    "\n",
    "        label = label * std + mean\n",
    "        pred = pred * std + mean\n",
    "\n",
    "        loss = torch.abs(pred - label) / (label + EPS)\n",
    "\n",
    "        return loss.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
